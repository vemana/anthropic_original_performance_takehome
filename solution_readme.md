# Performance

1345 cycles.

```text
[04:27:24] [lsv@vemana]$ python3 tests/submission_tests.py > /tmp/log.txt && tail -n 20 /tmp/log.txt
.........
----------------------------------------------------------------------
Ran 9 tests in 1.334s

OK
Kernel for H = 10, batch_size = 256, rounds = 16
Using 32 concurrent threads and found 1345 instructions.
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Testing forest_height=10, rounds=16, batch_size=256
CYCLES:  1345
Speedup over baseline:  109.83940520446096
```

```text
[/data/devel/vemana/anthropic/anthropic_original_performance_takehome]
[05:30:06] [lsv@vemana]$ git diff origin/main tests/

[/data/devel/vemana/anthropic/anthropic_original_performance_takehome]

```

# Correctness

Tests robustness with a newly-created exhaustive test suite in `perf_takehome.py` named `test_exhaustive_kernel_cycles`.


# Approach

Tools
- Understand Little's law on throughput [my 10-minute-mental-model](https://openparens.pages.dev/blog/2025/10mmm-littles-law/)
- Create a simple througput oriented programming language with global and per-thread context. Call this `XYZ` language. Similar to CUDA.
- Write a compiler for this language targeting the machine of this problem
- Write an optimizer alongside the compiler
- Create a visualizer for the compiled code
- Print stats convenient for throughput analysis
- The brain 


Approach
- Start with the approximation `Cycles ~ Pipeline depth + inverse throughput`
- Calculate the VALU, ALU, LOAD and FLOW engine budget
- Don't have budget for `load` for 16 rounds --> First three levels should not use load
- Simplify the hash calcuation using `multiply_add`
- Once the bottleneck is `load`, try to always saturate `LOAD` engine
- Look at the visualizer to improve saturation


Interesting Files
- `xyz_program.txt` contains the program (in `XYZ` language). Generated by `kernel_builder.py`
- [visual_instructions.html](https://htmlpreview.github.io/?https://raw.githubusercontent.com/vemana/anthropic_original_performance_takehome/refs/heads/main/visual_instructions.html) contains a visualization of instruction packing
- `prompt_parser.txt` contains the hand-written Grammar (a PEG style grammar) and the base prompt for generating the parser
- `kernel_builder.py` contains the workflow going from input to machine code
- `program_to_graph.py` converts a parsed program AST into a dependency graph
- `instr_graph_model.py` is the optimizer. It performs dependency analysis, reorder instructions and splits large-word ops into single-word ops
- `display.py` is the visualizer API


Assumptions
- Avoid overfitting the specific problem shape and target robustness (e.g. correctness checks via `test_exhaustive_kernel_cycles`)
- Assumption wrt problem shape
  - Input is a complete binary tree
  - `batch_size` is a multiple of VLEN (can be fixed; but lazy)
  - Affect the program generation in `XYZ` language but not that language itself. 
- Hacks specific to problem shape (`10, 256, 16`)
  - There's one hack in the optimizer that targets the test shape. This can be inferred at the cost of extra passes. I didn't want to implement it.
  - The optimizer's dependency analysis ignores pointer aliasing in establishing safety


Help used
- Display is generated by [Gemini session](https://gemini.google.com/share/076340d6d2e2) which starts with a base prompt and follows up with additional modifications
- Parser is generated from hand-written Grammar by [Gemini session](https://gemini.google.com/share/0bd587ebade2). Base prompt + follow ups + inline modifications


Interesting details

- The `XYZ` language has CUDA like semantics and runs with a number of concurrent threads
- The program calculates the max number of concurrent threads based on scratch space supply (fixed by this problem) and demand (implied by the program)
- Dependency analysis on scratch space is accurate and enables instruction reordering and splitting a VALU instruction into ALU instructions
  - Ignores pointer aliasing because it is tough and it is not needed for our toy program
- PEG style grammar for `XYZ` language, in order to add features quickly. Specifically ordered choice is the user-friendly feature of PEG
- Generally functional style code. Python's lack of first-class union types is a bummer
- The visualization tool was very handy and Gemini did a terrific job of one-shotting it
- The parser was harder for Gemini because of the myriad of detail but it eventually did a good job after repeatedly fixing the prompt to tell it one more thing

